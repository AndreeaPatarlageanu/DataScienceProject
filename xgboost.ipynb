{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b307236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports will be here:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import import_and_transform\n",
    "from utils import evaluate_model\n",
    "from utils import aggregate\n",
    "from utils import evaluate_model_log_reg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, precision_recall_curve)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0695589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_and_transform(data):\n",
    "    \"\"\"Import and basic preprocessing only.\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_parquet(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    df = df[df['userId'] != '']\n",
    "    df['userId'] = df['userId'].astype(int)\n",
    "    df[\"gender\"] = df[\"gender\"].map({'F': 0, 'M': 1})\n",
    "    df[\"level\"] = df[\"level\"].map({'free': 0, 'paid': 1})\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
    "    df['registration'] = pd.to_datetime(df['registration'])\n",
    "    \n",
    "    df['session_length'] = df.groupby(['userId', 'sessionId'])['ts'].transform(\n",
    "        lambda x: (x.max() - x.min()).total_seconds()\n",
    "    )\n",
    "    df['song_played'] = (df['page'] == 'NextSong').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_features(data, observation_end):\n",
    "    \"\"\"Calculate features using only data up to observation_end.\"\"\"\n",
    "    observation_end = pd.Timestamp(observation_end)\n",
    "    \n",
    "    user_df = data.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'registration': 'first',\n",
    "        'level': lambda x: x.mode().iloc[0] if not x.mode().empty else 0,\n",
    "        'sessionId': 'nunique',\n",
    "        'itemInSession': 'max',\n",
    "        'ts': ['min', 'max'],\n",
    "        'session_length': 'mean',\n",
    "        'song_played': 'sum',\n",
    "        'artist': pd.Series.nunique,\n",
    "        'length': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_df.columns = ['userId', 'gender', 'registration', 'level',\n",
    "                       'num_sessions', 'max_item_in_session', 'ts_min', 'ts_max', \n",
    "                       'avg_session_length_seconds', 'num_songs_played', \n",
    "                       'unique_artists', 'total_length']\n",
    "    \n",
    "    user_df['days_active'] = (observation_end - user_df['ts_min']).dt.days\n",
    "    user_df['membership_length'] = (observation_end - user_df['registration']).dt.days\n",
    "    user_df['days_since_last_activity'] = (observation_end - user_df['ts_max']).dt.days\n",
    "    user_df['songs_per_day'] = user_df['num_songs_played'] / (user_df['days_active'] + 1)\n",
    "    user_df['sessions_per_day'] = user_df['num_sessions'] / (user_df['days_active'] + 1)\n",
    "    \n",
    "    user_df = user_df.fillna(0)\n",
    "    user_df.set_index('userId', inplace=True)\n",
    "    \n",
    "    return user_df\n",
    "\n",
    "\n",
    "def get_churned_users(df, start_date, end_date):\n",
    "    \"\"\"Get users who churned between start_date and end_date.\"\"\"\n",
    "    start = pd.Timestamp(start_date)\n",
    "    end = pd.Timestamp(end_date)\n",
    "    \n",
    "    cancellations = df[df['page'] == 'Cancellation Confirmation']\n",
    "    churned = cancellations[(cancellations['ts'] > start) & \n",
    "                           (cancellations['ts'] <= end)]['userId'].unique()\n",
    "    return set(churned)\n",
    "\n",
    "\n",
    "# Load training data\n",
    "df_train = import_and_transform('Data/train.parquet')\n",
    "\n",
    "# Prepare test data\n",
    "df_test = import_and_transform('Data/test.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "453a3bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'aggregate_features_improved' from 'utils' (c:\\Users\\andre\\Documents\\university\\MASTER\\Polytechnique\\Python_for_data_science\\DataScienceProject\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aggregate_features_improved\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Createing observation dates every 5 days\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create multiple training samples with sliding window\u001b[39;00m\n\u001b[0;32m      4\u001b[0m training_dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2018-10-15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2018-11-05\u001b[39m\u001b[38;5;124m\"\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'aggregate_features_improved' from 'utils' (c:\\Users\\andre\\Documents\\university\\MASTER\\Polytechnique\\Python_for_data_science\\DataScienceProject\\utils.py)"
     ]
    }
   ],
   "source": [
    "from utils import aggregate_features_improved\n",
    "# Createing observation dates every 5 days\n",
    "# Create multiple training samples with sliding window\n",
    "training_dates = pd.date_range(\"2018-10-15\", \"2018-11-05\", freq=\"5D\")\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# For each observ date, we create a separate training sample:\n",
    "for obs_date in training_dates:\n",
    "\n",
    "    # Filtering data up to the observation date\n",
    "    df_obs = df_train[df_train[\"ts\"] <= obs_date]\n",
    "    features = aggregate_features_improved(df_obs, obs_date)\n",
    "\n",
    "    # Creating a 10 day window after the obervation date\n",
    "    # And we identify who churned in that period\n",
    "    window_end = obs_date + pd.Timedelta(days=10)\n",
    "    churned_users = get_churned_users(df_train, obs_date, window_end)\n",
    "\n",
    "    # 1 if they churned in the next 10 days, 0 otherwise\n",
    "    labels = pd.Series(\n",
    "        features.index.isin(churned_users).astype(int),\n",
    "        index=features.index,\n",
    "        name=\"churned\",\n",
    "    )\n",
    "\n",
    "    X_train_list.append(features)\n",
    "    y_train_list.append(labels)\n",
    "\n",
    "    print(\n",
    "        f\"Date of the observation: {obs_date.date()}, with {len(features)} users, and a {labels.mean():.2%} churn rate\"\n",
    "    )\n",
    "\n",
    "# We combine all observation windows:\n",
    "X_train_combined = pd.concat(X_train_list)\n",
    "y_train_combined = pd.concat(y_train_list)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "feature_cols = X_train_combined.select_dtypes(include=[np.number]).columns\n",
    "feature_cols = [\n",
    "    c\n",
    "    for c in feature_cols\n",
    "    if c not in [\"registration\", \"ts_min\", \"ts_max\", \"total_length\"]\n",
    "]\n",
    "\n",
    "X_train_final = X_train_combined[feature_cols]\n",
    "\n",
    "test_features = aggregate_features_improved(df_test, \"2018-11-20\")\n",
    "X_test = test_features[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dd09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\andre\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Scale pos weight: 21.53\n",
      "\n",
      "ðŸ” Starting XGBoost Grid Search (this may take 5-10 minutes)...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "âœ… Grid Search Complete!\n",
      "Best params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'scale_pos_weight': 21.53161652218256}\n",
      "Best CV ROC-AUC score: 0.7044\n",
      "Base predicted churn: 51.07%\n",
      "Predicted churn at 0.5 threshold: 51.07%\n",
      "Submission saved to xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "scale_pos_weight = (y_train_combined == 0).sum() / (y_train_combined == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# MINIMAL GRID - Focus on most important parameters\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],                    # Tree depth (most important!)\n",
    "    'learning_rate': [0.05, 0.1],              # Step size (most important!)\n",
    "    'n_estimators': [200, 300],                # Number of trees\n",
    "    'scale_pos_weight': [scale_pos_weight]     # Fixed based on class imbalance\n",
    "}\n",
    "\n",
    "# Create XGBoost with good defaults for other parameters\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    subsample=0.8,              # Use 80% of data per tree (reduces overfitting)\n",
    "    colsample_bytree=0.8,       # Use 80% of features per tree\n",
    "    tree_method='hist',         # Much faster for large datasets\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Reduce CV folds for speed\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    xgb_base,\n",
    "    param_grid_xgb,\n",
    "    cv=3,                       # 3 folds instead of 5 (faster!)\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Starting XGBoost Grid Search (this may take 5-10 minutes)...\")\n",
    "grid_search_xgb.fit(X_train_final, y_train_combined)\n",
    "\n",
    "print(f\"\\nâœ… Grid Search Complete!\")\n",
    "print(f\"Best params: {grid_search_xgb.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC score: {grid_search_xgb.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "evaluate_model(best_xgb, X_test, 0.5, file_out='xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0958655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.7, 'scale_pos_weight': 21.53161652218256, 'reg_lambda': 1, 'reg_alpha': 0.01, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 2, 'learning_rate': 0.03, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "Best CV ROC-AUC: 0.6904\n",
      "Base predicted churn: 58.26%\n",
      "Predicted churn at 0.5 threshold: 58.26%\n",
      "Submission saved to xgboost_2.csv\n"
     ]
    }
   ],
   "source": [
    "# More comprehensive grid based on your best params\n",
    "param_grid_xgb_advanced = {\n",
    "    'max_depth': [2, 3, 4],                    # Your best was 3, try neighbors\n",
    "    'learning_rate': [0.03, 0.05, 0.07],       # Your best was 0.05, fine-tune around it\n",
    "    'n_estimators': [200, 300, 400],           # Your best was 200, try more trees\n",
    "    'min_child_weight': [1, 3, 5],             # Prevent overfitting\n",
    "    'gamma': [0, 0.1, 0.2],                    # Minimum loss reduction\n",
    "    'subsample': [0.7, 0.8, 0.9],              # Row sampling\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],       # Column sampling\n",
    "    'reg_alpha': [0, 0.01, 0.1],               # L1 regularization\n",
    "    'reg_lambda': [1, 1.5, 2],                 # L2 regularization\n",
    "    'scale_pos_weight': [scale_pos_weight]\n",
    "}\n",
    "\n",
    "xgb_advanced = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV for speed\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_advanced,\n",
    "    param_grid_xgb_advanced,\n",
    "    n_iter=50,              # Test 50 random combinations\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_final, y_train_combined)\n",
    "print(f\"Best params: {random_search.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC: {random_search.best_score_:.4f}\")\n",
    "\n",
    "best_model_2 = random_search.best_estimator_\n",
    "evaluate_model(best_model_2, X_test, 0.5, file_out ='xgboost_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0be725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
