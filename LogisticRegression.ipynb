{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be21dc6b",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91707aa2",
   "metadata": {},
   "source": [
    "In this notebook, we apply Logistic Regression to our data and we try to predict 'churn'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8dcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports will be here:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import import_and_transform\n",
    "from utils import evaluate_model\n",
    "from utils import aggregate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eee076",
   "metadata": {},
   "source": [
    "Based on the exploratory data analysis **EDA**, we will now modify our database accordingly. The EDA showed issues and necessary changed that require database modifications.\n",
    "\n",
    "We restructurate our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95583d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing data import and basic preprocessing:\n",
    "\n",
    "\n",
    "def import_and_transform(data):\n",
    "\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_parquet(data)\n",
    "    else:\n",
    "        df = data\n",
    "\n",
    "    # Remove invalid userIds and convert valid ones to integers\n",
    "    df = df[df[\"userId\"] != \"\"]\n",
    "    df[\"userId\"] = df[\"userId\"].astype(int)\n",
    "\n",
    "    # Ecnode the catgeorical variables\n",
    "    df[\"gender\"] = df[\"gender\"].map({\"F\": 0, \"M\": 1})\n",
    "    df[\"level\"] = df[\"level\"].map({\"free\": 0, \"paid\": 1})\n",
    "\n",
    "    # Convert the timestamps\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\")\n",
    "    df[\"registration\"] = pd.to_datetime(df[\"registration\"])\n",
    "\n",
    "    # For each user session, we compute the duration in seconds\n",
    "    df[\"session_length\"] = df.groupby([\"userId\", \"sessionId\"])[\"ts\"].transform(\n",
    "        lambda x: (x.max() - x.min()).total_seconds()\n",
    "    )\n",
    "\n",
    "    # Song play indicator:\n",
    "    df[\"song_played\"] = (df[\"page\"] == \"NextSong\").astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7545b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating event-level data into user lvl features:\n",
    "\n",
    "\n",
    "def aggregate_features(data, observation_end):\n",
    "\n",
    "    observation_end = pd.Timestamp(observation_end)\n",
    "\n",
    "    # Aggregate features at user level:\n",
    "    user_df = (\n",
    "        data.groupby(\"userId\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"gender\": \"first\",\n",
    "                \"registration\": \"first\",\n",
    "                \"level\": lambda x: x.mode().iloc[0] if not x.mode().empty else 0,\n",
    "                \"sessionId\": \"nunique\",\n",
    "                \"itemInSession\": \"max\",\n",
    "                \"ts\": [\"min\", \"max\"],\n",
    "                \"session_length\": \"mean\",\n",
    "                \"song_played\": \"sum\",\n",
    "                \"artist\": pd.Series.nunique,\n",
    "                \"length\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # A more intuitive column order and names:\n",
    "    user_df.columns = [\n",
    "        \"userId\",\n",
    "        \"gender\",\n",
    "        \"registration\",\n",
    "        \"level\",\n",
    "        \"num_sessions\",\n",
    "        \"max_item_in_session\",\n",
    "        \"ts_min\",\n",
    "        \"ts_max\",\n",
    "        \"avg_session_length_seconds\",\n",
    "        \"num_songs_played\",\n",
    "        \"unique_artists\",\n",
    "        \"total_length\",\n",
    "    ]\n",
    "\n",
    "    # Creating time based engagement metrics:\n",
    "\n",
    "    # Days from the first activity to the end of the obs period\n",
    "    user_df[\"days_active\"] = (observation_end - user_df[\"ts_min\"]).dt.days\n",
    "\n",
    "    # Total membership duration\n",
    "    user_df[\"membership_length\"] = (observation_end - user_df[\"registration\"]).dt.days\n",
    "\n",
    "    # Engagement rates:\n",
    "    user_df[\"days_since_last_activity\"] = (observation_end - user_df[\"ts_max\"]).dt.days\n",
    "    user_df[\"songs_per_day\"] = user_df[\"num_songs_played\"] / (\n",
    "        user_df[\"days_active\"] + 1\n",
    "    )\n",
    "    user_df[\"sessions_per_day\"] = user_df[\"num_sessions\"] / (user_df[\"days_active\"] + 1)\n",
    "\n",
    "    # Fill missing values with 0s\n",
    "    user_df = user_df.fillna(0)\n",
    "\n",
    "    # Set userId as index\n",
    "    user_df.set_index(\"userId\", inplace=True)\n",
    "\n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b204f2",
   "metadata": {},
   "source": [
    "Since in this competitions task we are asked to focus on the churn in a specific time window, we create a function which identifies who churned within a specified period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8b925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churned_users(df, start_date, end_date):\n",
    "\n",
    "    # Convert the given dates to timestamps\n",
    "    start = pd.Timestamp(start_date)\n",
    "    end = pd.Timestamp(end_date)\n",
    "\n",
    "    # Identify users who cancelled their subscirptions in the given period\n",
    "    cancellations = df[df[\"page\"] == \"Cancellation Confirmation\"]\n",
    "    churned = cancellations[\n",
    "        (cancellations[\"ts\"] > start) & (cancellations[\"ts\"] <= end)\n",
    "    ][\"userId\"].unique()\n",
    "\n",
    "    # Return as a set\n",
    "    return set(churned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c4a47",
   "metadata": {},
   "source": [
    "Now, we are ready to load the train and test data and apply these modifications directly on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbcd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_train = import_and_transform(\"Data/train.parquet\")\n",
    "\n",
    "# Prepare test data\n",
    "df_test = import_and_transform(\"Data/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab4245",
   "metadata": {},
   "source": [
    "We want to capture more temporal patterns. We want to \"teach\" the model to recognize churn patterns across different time periods. How do we do that?\n",
    "\n",
    "If we look at only one point in the time, we do not have enough examples to train the model. So instead of taking just one \"snapshot\", we take multiple snapshots at different times and we consider each one of them as individual prediction problems. \n",
    "\n",
    "Like this, we increase our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9f1716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of the observation: 2018-10-15, with 16271 users, and a 5.08% churn rate\n",
      "Date of the observation: 2018-10-20, with 17347 users, and a 4.48% churn rate\n",
      "Date of the observation: 2018-10-25, with 17888 users, and a 4.49% churn rate\n",
      "Date of the observation: 2018-10-30, with 18271 users, and a 4.46% churn rate\n",
      "Date of the observation: 2018-11-04, with 18592 users, and a 3.78% churn rate\n"
     ]
    }
   ],
   "source": [
    "# Createing observation dates every 5 days\n",
    "# Create multiple training samples with sliding window\n",
    "training_dates = pd.date_range(\"2018-10-15\", \"2018-11-05\", freq=\"5D\")\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# For each observ date, we create a separate training sample:\n",
    "for obs_date in training_dates:\n",
    "\n",
    "    # Filtering data up to the observation date\n",
    "    df_obs = df_train[df_train[\"ts\"] <= obs_date]\n",
    "    features = aggregate_features(df_obs, obs_date)\n",
    "\n",
    "    # Creating a 10 day window after the obervation date\n",
    "    # And we identify who churned in that period\n",
    "    window_end = obs_date + pd.Timedelta(days=10)\n",
    "    churned_users = get_churned_users(df_train, obs_date, window_end)\n",
    "\n",
    "    # 1 if they churned in the next 10 days, 0 otherwise\n",
    "    labels = pd.Series(\n",
    "        features.index.isin(churned_users).astype(int),\n",
    "        index=features.index,\n",
    "        name=\"churned\",\n",
    "    )\n",
    "\n",
    "    X_train_list.append(features)\n",
    "    y_train_list.append(labels)\n",
    "\n",
    "    print(\n",
    "        f\"Date of the observation: {obs_date.date()}, with {len(features)} users, and a {labels.mean():.2%} churn rate\"\n",
    "    )\n",
    "\n",
    "# We combine all observation windows:\n",
    "X_train_combined = pd.concat(X_train_list)\n",
    "y_train_combined = pd.concat(y_train_list)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "feature_cols = X_train_combined.select_dtypes(include=[np.number]).columns\n",
    "feature_cols = [\n",
    "    c\n",
    "    for c in feature_cols\n",
    "    if c not in [\"registration\", \"ts_min\", \"ts_max\", \"total_length\"]\n",
    "]\n",
    "\n",
    "X_train_final = X_train_combined[feature_cols]\n",
    "\n",
    "test_features = aggregate_features(df_test, \"2018-11-20\")\n",
    "X_test = test_features[feature_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f171ee",
   "metadata": {},
   "source": [
    "We are trying to predict 0/1 Yes/No churn. The very first intuitive step to do is to apply Logistic Regression and then optimize it.\n",
    "\n",
    "Hence, we start by applying a vanilla Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48f7942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base predicted churn: 31.96%\n",
      "Predicted churn at 0.5 threshold: 31.96%\n",
      "Submission saved to log_reg_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "log_reg.fit(X_train_final, y_train_combined)\n",
    "\n",
    "from utils import evaluate_model\n",
    "\n",
    "evaluate_model(log_reg, X_test, 0.5, file_out=\"log_reg_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdcdcd",
   "metadata": {},
   "source": [
    "In Logistic Regression, we have hyperprameters that we need to choose *before* the training so we control how the model learns. Our hyperparameters are:\n",
    "\n",
    "- **C**: how much we penalize complexity (too high is overfitting, too low is underfitting)\n",
    "- **penalty**: which type of regularization we use (l1 or l2)\n",
    "- **solver**: which optimization algorithm we use\n",
    "\n",
    "Since we do not know which combination of these 3 is the best, we need to try all of them to decide which one we'll use in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2ac8e",
   "metadata": {},
   "source": [
    "For solver, we use \"Liblinear\", and \"Saga\". A solver is an optimization algorithm that finds the best model param during training. **Saga** is fast, uses low memory and is built for big data. Moreover, it uses Stochastic Average Gradient descent, meaning it updates the model parameters using small bacthes of data at time, it does not load the whole dataset in memory from the beginning, and it converges fast on big data. **Liblinear** is more reliable and is a classic choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d74dc8",
   "metadata": {},
   "source": [
    "**ROC-AUC** (area under the curve) measures how well the model separates the 2 classes, despite the imbalance. So in our case, it is used to answer:\n",
    "\n",
    "\"What is the probability the model will rank the churner higher if we pick one churner and one non-churner at random?\"\n",
    "\n",
    "Possible scores:\n",
    "\n",
    "- $<0.5$ means wrose than random\n",
    "- $~0.5$ means random guessing\n",
    "- $1.0$ means perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4986a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     15\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     16\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mLogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     17\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# We train on all the combinations of parameters\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_final, y_train_combined)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We define the param grid\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 1, 5, 50], # Regularization strength\n",
    "    \"penalty\": [\"l1\", \"l2\"], # l1 = Lasso, l2 = Ridge\n",
    "    \"solver\": [\"liblinear\", \"saga\"], # Optim algorithm\n",
    "    \"class_weight\": [\"balanced\"], # Handle class imbalance\n",
    "    \"max_iter\": [1000]\n",
    "}\n",
    "\n",
    "# We use cross-validation, because our dataset is imbalanced\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# We perform grid search over all the combinations\n",
    "# We optimize for ROC-AUC\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# We train on all the combinations of parameters\n",
    "grid_search.fit(X_train_final, y_train_combined)\n",
    "\n",
    "print()\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Let's save the best model now:\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274e352",
   "metadata": {},
   "source": [
    "We found the best parameters and we will use them to evaluate on the test set.\n",
    "\n",
    "We got the best ROC-AUC score of 70%, which means that the model learned some real churn patters. However, 30% will miss some churners. The ideal ROC-AUC score for churn prediction should be around 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e0546",
   "metadata": {},
   "source": [
    "Now that we have the best model, let's evaluate it on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036306a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base predicted churn: 61.78%\n",
      "Predicted churn at 0.5 threshold: 61.78%\n",
      "Submission saved to log_reg_2.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(best_model, X_test, 0.5, file_out=\"log_reg_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88416fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of the observation: 2018-10-15, with 16271 users, and a 5.08% churn rate\n",
      "Date of the observation: 2018-10-20, with 17347 users, and a 4.48% churn rate\n",
      "Date of the observation: 2018-10-25, with 17888 users, and a 4.49% churn rate\n",
      "Date of the observation: 2018-10-30, with 18271 users, and a 4.46% churn rate\n",
      "Date of the observation: 2018-11-04, with 18592 users, and a 3.78% churn rate\n"
     ]
    }
   ],
   "source": [
    "from utils import aggregate_features_improved\n",
    "# Createing observation dates every 5 days\n",
    "# Create multiple training samples with sliding window\n",
    "training_dates = pd.date_range(\"2018-10-15\", \"2018-11-05\", freq=\"5D\")\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# For each observ date, we create a separate training sample:\n",
    "for obs_date in training_dates:\n",
    "\n",
    "    # Filtering data up to the observation date\n",
    "    df_obs = df_train[df_train[\"ts\"] <= obs_date]\n",
    "    features = aggregate_features_improved(df_obs, obs_date)\n",
    "\n",
    "    # Creating a 10 day window after the obervation date\n",
    "    # And we identify who churned in that period\n",
    "    window_end = obs_date + pd.Timedelta(days=10)\n",
    "    churned_users = get_churned_users(df_train, obs_date, window_end)\n",
    "\n",
    "    # 1 if they churned in the next 10 days, 0 otherwise\n",
    "    labels = pd.Series(\n",
    "        features.index.isin(churned_users).astype(int),\n",
    "        index=features.index,\n",
    "        name=\"churned\",\n",
    "    )\n",
    "\n",
    "    X_train_list.append(features)\n",
    "    y_train_list.append(labels)\n",
    "\n",
    "    print(\n",
    "        f\"Date of the observation: {obs_date.date()}, with {len(features)} users, and a {labels.mean():.2%} churn rate\"\n",
    "    )\n",
    "\n",
    "# We combine all observation windows:\n",
    "X_train_combined = pd.concat(X_train_list)\n",
    "y_train_combined = pd.concat(y_train_list)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "feature_cols = X_train_combined.select_dtypes(include=[np.number]).columns\n",
    "feature_cols = [\n",
    "    c\n",
    "    for c in feature_cols\n",
    "    if c not in [\"registration\", \"ts_min\", \"ts_max\", \"total_length\"]\n",
    "]\n",
    "\n",
    "X_train_final = X_train_combined[feature_cols]\n",
    "\n",
    "test_features = aggregate_features_improved(df_test, \"2018-11-20\")\n",
    "X_test = test_features[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad9873b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>level_first</th>\n",
       "      <th>level_current</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>avg_session_length</th>\n",
       "      <th>num_songs_played</th>\n",
       "      <th>unique_artists</th>\n",
       "      <th>unique_songs</th>\n",
       "      <th>avg_song_length</th>\n",
       "      <th>days_active</th>\n",
       "      <th>...</th>\n",
       "      <th>downgrade_attempts</th>\n",
       "      <th>upgrade_attempts</th>\n",
       "      <th>total_page_views</th>\n",
       "      <th>has_social_activity</th>\n",
       "      <th>positive_actions</th>\n",
       "      <th>satisfaction_ratio</th>\n",
       "      <th>engagement_rate</th>\n",
       "      <th>problem_signals</th>\n",
       "      <th>ads_per_song</th>\n",
       "      <th>actions_per_session</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000025</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>56774.847498</td>\n",
       "      <td>1030</td>\n",
       "      <td>795</td>\n",
       "      <td>935</td>\n",
       "      <td>249.797113</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1259</td>\n",
       "      <td>1</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.131911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>104.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12457.375000</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>253.653276</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>17.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000083</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>22024.669463</td>\n",
       "      <td>501</td>\n",
       "      <td>427</td>\n",
       "      <td>478</td>\n",
       "      <td>244.723096</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>596</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.077689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>49.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9744.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>250.488380</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>25.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11326.024096</td>\n",
       "      <td>127</td>\n",
       "      <td>119</td>\n",
       "      <td>125</td>\n",
       "      <td>244.640807</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>20.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         gender  level_first  level_current  num_sessions  avg_session_length  \\\n",
       "userId                                                                          \n",
       "1000025       1            0              1            11        56774.847498   \n",
       "1000035       0            0              0             4        12457.375000   \n",
       "1000083       1            0              1            11        22024.669463   \n",
       "1000103       0            0              1             1         9744.000000   \n",
       "1000164       0            0              0             7        11326.024096   \n",
       "\n",
       "         num_songs_played  unique_artists  unique_songs  avg_song_length  \\\n",
       "userId                                                                     \n",
       "1000025              1030             795           935       249.797113   \n",
       "1000035                66              66            66       253.653276   \n",
       "1000083               501             427           478       244.723096   \n",
       "1000103                39              39            39       250.488380   \n",
       "1000164               127             119           125       244.640807   \n",
       "\n",
       "         days_active  ...  downgrade_attempts  upgrade_attempts  \\\n",
       "userId                ...                                         \n",
       "1000025           12  ...                14.0               2.0   \n",
       "1000035            9  ...                 0.0               1.0   \n",
       "1000083           13  ...                 2.0               4.0   \n",
       "1000103           10  ...                 0.0               2.0   \n",
       "1000164           13  ...                 0.0               0.0   \n",
       "\n",
       "         total_page_views  has_social_activity  positive_actions  \\\n",
       "userId                                                             \n",
       "1000025              1259                    1             121.0   \n",
       "1000035                88                    0               0.0   \n",
       "1000083               596                    1              35.0   \n",
       "1000103                51                    0               0.0   \n",
       "1000164               166                    1              13.0   \n",
       "\n",
       "         satisfaction_ratio  engagement_rate  problem_signals  ads_per_song  \\\n",
       "userId                                                                        \n",
       "1000025            0.864198         0.131911              0.0      0.003880   \n",
       "1000035            0.000000         0.000000              0.0      0.029851   \n",
       "1000083            0.875000         0.077689              0.0      0.015936   \n",
       "1000103            0.000000         0.000000              0.0      0.075000   \n",
       "1000164            0.000000         0.000000              0.0      0.140625   \n",
       "\n",
       "         actions_per_session  \n",
       "userId                        \n",
       "1000025           104.916667  \n",
       "1000035            17.600000  \n",
       "1000083            49.666667  \n",
       "1000103            25.500000  \n",
       "1000164            20.750000  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bffdfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base predicted churn: 53.06%\n",
      "Predicted churn at 0.5 threshold: 53.06%\n",
      "Submission saved to new_log_reg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "log_reg.fit(X_train_final, y_train_combined)\n",
    "evaluate_model(log_reg, X_test, 0.5, file_out='new_log_reg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f4c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
