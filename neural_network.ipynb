{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392a7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports will be here:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import import_and_transform\n",
    "from utils import evaluate_model\n",
    "from utils import aggregate\n",
    "from utils import evaluate_model_log_reg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, precision_recall_curve)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f3926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_and_transform(data):\n",
    "    \"\"\"Import and basic preprocessing only.\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_parquet(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    df = df[df['userId'] != '']\n",
    "    df['userId'] = df['userId'].astype(int)\n",
    "    df[\"gender\"] = df[\"gender\"].map({'F': 0, 'M': 1})\n",
    "    df[\"level\"] = df[\"level\"].map({'free': 0, 'paid': 1})\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
    "    df['registration'] = pd.to_datetime(df['registration'])\n",
    "    \n",
    "    df['session_length'] = df.groupby(['userId', 'sessionId'])['ts'].transform(\n",
    "        lambda x: (x.max() - x.min()).total_seconds()\n",
    "    )\n",
    "    df['song_played'] = (df['page'] == 'NextSong').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_features(data, observation_end):\n",
    "    \"\"\"Calculate features using only data up to observation_end.\"\"\"\n",
    "    observation_end = pd.Timestamp(observation_end)\n",
    "    \n",
    "    user_df = data.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'registration': 'first',\n",
    "        'level': lambda x: x.mode().iloc[0] if not x.mode().empty else 0,\n",
    "        'sessionId': 'nunique',\n",
    "        'itemInSession': 'max',\n",
    "        'ts': ['min', 'max'],\n",
    "        'session_length': 'mean',\n",
    "        'song_played': 'sum',\n",
    "        'artist': pd.Series.nunique,\n",
    "        'length': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_df.columns = ['userId', 'gender', 'registration', 'level',\n",
    "                       'num_sessions', 'max_item_in_session', 'ts_min', 'ts_max', \n",
    "                       'avg_session_length_seconds', 'num_songs_played', \n",
    "                       'unique_artists', 'total_length']\n",
    "    \n",
    "    user_df['days_active'] = (observation_end - user_df['ts_min']).dt.days\n",
    "    user_df['membership_length'] = (observation_end - user_df['registration']).dt.days\n",
    "    user_df['days_since_last_activity'] = (observation_end - user_df['ts_max']).dt.days\n",
    "    user_df['songs_per_day'] = user_df['num_songs_played'] / (user_df['days_active'] + 1)\n",
    "    user_df['sessions_per_day'] = user_df['num_sessions'] / (user_df['days_active'] + 1)\n",
    "    \n",
    "    user_df = user_df.fillna(0)\n",
    "    user_df.set_index('userId', inplace=True)\n",
    "    \n",
    "    return user_df\n",
    "\n",
    "\n",
    "def get_churned_users(df, start_date, end_date):\n",
    "    \"\"\"Get users who churned between start_date and end_date.\"\"\"\n",
    "    start = pd.Timestamp(start_date)\n",
    "    end = pd.Timestamp(end_date)\n",
    "    \n",
    "    cancellations = df[df['page'] == 'Cancellation Confirmation']\n",
    "    churned = cancellations[(cancellations['ts'] > start) & \n",
    "                           (cancellations['ts'] <= end)]['userId'].unique()\n",
    "    return set(churned)\n",
    "\n",
    "\n",
    "# Load training data\n",
    "df_train = import_and_transform('Data/train.parquet')\n",
    "\n",
    "# Prepare test data\n",
    "df_test = import_and_transform('Data/test.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb12c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs date: 2018-10-15, Users: 16271, Churn rate: 5.08%\n",
      "Obs date: 2018-10-20, Users: 17347, Churn rate: 4.48%\n",
      "Obs date: 2018-10-25, Users: 17888, Churn rate: 4.49%\n",
      "Obs date: 2018-10-30, Users: 18271, Churn rate: 4.46%\n",
      "Obs date: 2018-11-04, Users: 18592, Churn rate: 3.78%\n"
     ]
    }
   ],
   "source": [
    "# Create multiple training samples with sliding window\n",
    "training_dates = pd.date_range('2018-10-15', '2018-11-05', freq='5D')\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for obs_date in training_dates:\n",
    "    # Features from data up to obs_date\n",
    "    df_obs = df_train[df_train['ts'] <= obs_date]\n",
    "    features = aggregate_features(df_obs, obs_date)\n",
    "    \n",
    "    # Labels from next 10 days\n",
    "    window_end = obs_date + pd.Timedelta(days=10)\n",
    "    churned_users = get_churned_users(df_train, obs_date, window_end)\n",
    "    \n",
    "    # Convert to Series with same index as features\n",
    "    labels = pd.Series(\n",
    "        features.index.isin(churned_users).astype(int),\n",
    "        index=features.index,\n",
    "        name='churned'\n",
    "    )\n",
    "    \n",
    "    X_train_list.append(features)\n",
    "    y_train_list.append(labels)\n",
    "    \n",
    "    print(f\"Obs date: {obs_date.date()}, Users: {len(features)}, Churn rate: {labels.mean():.2%}\")\n",
    "\n",
    "# Combine all training samples\n",
    "X_train_combined = pd.concat(X_train_list)\n",
    "y_train_combined = pd.concat(y_train_list)\n",
    "\n",
    "# Drop non-feature columns\n",
    "feature_cols = X_train_combined.select_dtypes(include=[np.number]).columns\n",
    "feature_cols = [c for c in feature_cols if c not in ['registration', 'ts_min', 'ts_max', 'total_length']]\n",
    "X_train_final = X_train_combined[feature_cols]\n",
    "\n",
    "test_features = aggregate_features(df_test, '2018-11-20')\n",
    "X_test = test_features[feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d215e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.9.1-cp312-cp312-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/110.9 MB 3.5 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 0.8/110.9 MB 8.5 MB/s eta 0:00:13\n",
      "    --------------------------------------- 1.5/110.9 MB 11.6 MB/s eta 0:00:10\n",
      "    --------------------------------------- 2.0/110.9 MB 11.8 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 2.8/110.9 MB 12.1 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 3.4/110.9 MB 12.1 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.1/110.9 MB 12.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.8/110.9 MB 12.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.4/110.9 MB 13.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 5.8/110.9 MB 12.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 6.6/110.9 MB 13.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 7.1/110.9 MB 13.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 8.1/110.9 MB 13.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 8.7/110.9 MB 13.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 9.5/110.9 MB 13.7 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 10.2/110.9 MB 13.8 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 10.9/110.9 MB 14.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 11.6/110.9 MB 14.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.3/110.9 MB 14.9 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.9/110.9 MB 14.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 13.9/110.9 MB 15.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 14.4/110.9 MB 14.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 15.3/110.9 MB 15.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 16.0/110.9 MB 16.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 16.9/110.9 MB 16.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 17.7/110.9 MB 16.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 18.4/110.9 MB 16.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 19.1/110.9 MB 16.0 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 19.9/110.9 MB 16.0 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 20.7/110.9 MB 16.4 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 21.5/110.9 MB 16.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 22.3/110.9 MB 16.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 23.1/110.9 MB 16.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 24.0/110.9 MB 16.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 24.8/110.9 MB 17.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 25.6/110.9 MB 17.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 26.4/110.9 MB 17.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 27.0/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 27.8/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 28.6/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 29.2/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 30.0/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 30.7/110.9 MB 16.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 31.5/110.9 MB 16.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 32.3/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 33.2/110.9 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 34.0/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 34.6/110.9 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 35.5/110.9 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 36.4/110.9 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 37.2/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 37.9/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 38.7/110.9 MB 16.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 39.5/110.9 MB 17.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 40.3/110.9 MB 17.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 41.2/110.9 MB 17.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 41.9/110.9 MB 17.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 42.8/110.9 MB 17.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 43.7/110.9 MB 17.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 44.6/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 45.4/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 46.2/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 47.1/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 47.8/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 48.8/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 49.6/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 50.4/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 51.2/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 51.9/110.9 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 52.6/110.9 MB 16.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 53.2/110.9 MB 16.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 54.1/110.9 MB 16.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 54.9/110.9 MB 16.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 55.4/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 56.3/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 56.7/110.9 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 57.5/110.9 MB 15.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 58.5/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 59.3/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 60.1/110.9 MB 15.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 61.0/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 61.8/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 62.6/110.9 MB 16.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 63.4/110.9 MB 16.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 64.2/110.9 MB 16.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 65.0/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 65.9/110.9 MB 16.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 67.0/110.9 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 67.7/110.9 MB 17.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 68.4/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 69.2/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 69.9/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 70.7/110.9 MB 16.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 71.6/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 72.4/110.9 MB 17.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 73.3/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 74.1/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 74.9/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 75.8/110.9 MB 17.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 76.6/110.9 MB 17.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 77.2/110.9 MB 16.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 78.0/110.9 MB 17.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 78.7/110.9 MB 17.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 79.4/110.9 MB 16.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 80.0/110.9 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 80.7/110.9 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 81.6/110.9 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 82.1/110.9 MB 16.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 82.8/110.9 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 83.6/110.9 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 83.9/110.9 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 84.8/110.9 MB 15.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 85.7/110.9 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 86.5/110.9 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 87.1/110.9 MB 14.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 87.8/110.9 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 88.7/110.9 MB 14.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 89.3/110.9 MB 14.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 90.4/110.9 MB 15.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 91.1/110.9 MB 15.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 91.7/110.9 MB 15.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.6/110.9 MB 15.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 93.3/110.9 MB 15.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 94.2/110.9 MB 16.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 95.1/110.9 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 96.0/110.9 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 96.9/110.9 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 97.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 98.7/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 99.5/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 100.1/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 101.1/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 101.8/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 102.7/110.9 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.5/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 104.4/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 105.2/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 106.1/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 106.8/110.9 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 107.5/110.9 MB 17.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  108.5/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.3/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.1/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.9/110.9 MB 9.5 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed sympy-1.14.0 torch-2.9.1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch scikit-learn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     _load_dll_libraries()\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "!pip install torch scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "scale_pos_weight = (y_train_combined == 0).sum() / (y_train_combined == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Split for validation\n",
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train_final, y_train_combined, test_size=0.2, random_state=42, stratify=y_train_combined\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_nn)\n",
    "y_train_tensor = torch.FloatTensor(y_train_nn.values).reshape(-1, 1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_nn)\n",
    "y_val_tensor = torch.FloatTensor(y_val_nn.values).reshape(-1, 1)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Define Neural Network (same architecture as your TensorFlow version)\n",
    "class ChurnNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=128, dropout_rate=0.3):\n",
    "        super(ChurnNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_units)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_units)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_units, hidden_units // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_units // 2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layer3 = nn.Linear(hidden_units // 2, hidden_units // 4)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate / 2)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_units // 4, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ChurnNN(input_size=X_train_final.shape[1], hidden_units=128, dropout_rate=0.3).to(device)\n",
    "\n",
    "# Calculate class weights\n",
    "pos_weight = torch.FloatTensor([scale_pos_weight]).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device, pos_weight):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Apply class weights manually\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Weight the loss for positive class\n",
    "        weights = torch.where(y_batch == 1, pos_weight, torch.ones_like(y_batch))\n",
    "        loss = (criterion(outputs, y_batch) * weights.squeeze()).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(actuals, predictions)\n",
    "    return auc\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nðŸ” Training Neural Network...\")\n",
    "best_auc = 0\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, pos_weight)\n",
    "    val_auc = validate(model, val_loader, device)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100 | Loss: {train_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_nn_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"Best validation AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_nn_model.pth'))\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE WRAPPER FOR evaluate_model\n",
    "# ============================================================================\n",
    "\n",
    "class PyTorchModelWrapper:\n",
    "    \"\"\"Wrapper to make PyTorch model compatible with evaluate_model\"\"\"\n",
    "    \n",
    "    def __init__(self, pytorch_model, device):\n",
    "        self.model = pytorch_model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probabilities in sklearn format\"\"\"\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "            proba_class_1 = self.model(X_tensor).cpu().numpy().flatten()\n",
    "            proba_class_0 = 1 - proba_class_1\n",
    "            return np.column_stack([proba_class_0, proba_class_1])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return binary predictions\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# Wrap the model\n",
    "wrapped_nn_model = PyTorchModelWrapper(model, device)\n",
    "\n",
    "# Use evaluate_model\n",
    "print(\"\\nðŸ“Š Evaluating Neural Network...\")\n",
    "evaluate_model(wrapped_nn_model, X_test, 0.5, file_out='nn_model.csv')\n",
    "\n",
    "# ============================================================================\n",
    "# BONUS: Find optimal threshold\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get validation predictions\n",
    "with torch.no_grad():\n",
    "    X_val_tensor = X_val_tensor.to(device)\n",
    "    y_proba_val = model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Find optimal threshold\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_nn, y_proba_val)\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimal threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Evaluate with optimal threshold\n",
    "evaluate_model(wrapped_nn_model, X_test, optimal_threshold, file_out='nn_model_optimal.csv')\n",
    "\n",
    "print(\"\\nâœ… Neural Network Complete!\")\n",
    "print(f\"Created: nn_model.csv (threshold=0.5)\")\n",
    "print(f\"Created: nn_model_optimal.csv (threshold={optimal_threshold:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e8793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
