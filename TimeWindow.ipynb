{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f3cb86-0e0e-40be-b071-df695776a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs date: 2018-10-15, Users: 16271, Churn rate: 5.08%\n",
      "Obs date: 2018-10-20, Users: 17347, Churn rate: 4.48%\n",
      "Obs date: 2018-10-25, Users: 17888, Churn rate: 4.49%\n",
      "Obs date: 2018-10-30, Users: 18271, Churn rate: 4.46%\n",
      "Obs date: 2018-11-04, Users: 18592, Churn rate: 3.78%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def import_and_transform(data):\n",
    "    \"\"\"Import and basic preprocessing only.\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_parquet(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    df = df[df['userId'] != '']\n",
    "    df['userId'] = df['userId'].astype(int)\n",
    "    df[\"gender\"] = df[\"gender\"].map({'F': 0, 'M': 1})\n",
    "    df[\"level\"] = df[\"level\"].map({'free': 0, 'paid': 1})\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
    "    df['registration'] = pd.to_datetime(df['registration'])\n",
    "    \n",
    "    df['session_length'] = df.groupby(['userId', 'sessionId'])['ts'].transform(\n",
    "        lambda x: (x.max() - x.min()).total_seconds()\n",
    "    )\n",
    "    df['song_played'] = (df['page'] == 'NextSong').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_features(data, observation_end):\n",
    "    \"\"\"Calculate features using only data up to observation_end.\"\"\"\n",
    "    observation_end = pd.Timestamp(observation_end)\n",
    "    \n",
    "    user_df = data.groupby('userId').agg({\n",
    "        'gender': 'first',\n",
    "        'registration': 'first',\n",
    "        'level': lambda x: x.mode().iloc[0] if not x.mode().empty else 0,\n",
    "        'sessionId': 'nunique',\n",
    "        'itemInSession': 'max',\n",
    "        'ts': ['min', 'max'],\n",
    "        'session_length': 'mean',\n",
    "        'song_played': 'sum',\n",
    "        'artist': pd.Series.nunique,\n",
    "        'length': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_df.columns = ['userId', 'gender', 'registration', 'level',\n",
    "                       'num_sessions', 'max_item_in_session', 'ts_min', 'ts_max', \n",
    "                       'avg_session_length_seconds', 'num_songs_played', \n",
    "                       'unique_artists', 'total_length']\n",
    "    \n",
    "    user_df['days_active'] = (observation_end - user_df['ts_min']).dt.days\n",
    "    user_df['membership_length'] = (observation_end - user_df['registration']).dt.days\n",
    "    user_df['days_since_last_activity'] = (observation_end - user_df['ts_max']).dt.days\n",
    "    user_df['songs_per_day'] = user_df['num_songs_played'] / (user_df['days_active'] + 1)\n",
    "    user_df['sessions_per_day'] = user_df['num_sessions'] / (user_df['days_active'] + 1)\n",
    "    \n",
    "    user_df = user_df.fillna(0)\n",
    "    user_df.set_index('userId', inplace=True)\n",
    "    \n",
    "    return user_df\n",
    "\n",
    "\n",
    "def get_churned_users(df, start_date, end_date):\n",
    "    \"\"\"Get users who churned between start_date and end_date.\"\"\"\n",
    "    start = pd.Timestamp(start_date)\n",
    "    end = pd.Timestamp(end_date)\n",
    "    \n",
    "    cancellations = df[df['page'] == 'Cancellation Confirmation']\n",
    "    churned = cancellations[(cancellations['ts'] > start) & \n",
    "                           (cancellations['ts'] <= end)]['userId'].unique()\n",
    "    return set(churned)\n",
    "\n",
    "\n",
    "# Load training data\n",
    "df_train = import_and_transform('Data/train.parquet')\n",
    "\n",
    "# Prepare test data\n",
    "df_test = import_and_transform('Data/test.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7765ddf1-24f8-4c43-b10a-06ca39f44580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs date: 2018-10-15, Users: 16271, Churn rate: 5.08%\n",
      "Obs date: 2018-10-20, Users: 17347, Churn rate: 4.48%\n",
      "Obs date: 2018-10-25, Users: 17888, Churn rate: 4.49%\n",
      "Obs date: 2018-10-30, Users: 18271, Churn rate: 4.46%\n",
      "Obs date: 2018-11-04, Users: 18592, Churn rate: 3.78%\n"
     ]
    }
   ],
   "source": [
    "# Create multiple training samples with sliding window\n",
    "training_dates = pd.date_range('2018-10-15', '2018-11-05', freq='5D')\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for obs_date in training_dates:\n",
    "    # Features from data up to obs_date\n",
    "    df_obs = df_train[df_train['ts'] <= obs_date]\n",
    "    features = aggregate_features(df_obs, obs_date)\n",
    "    \n",
    "    # Labels from next 10 days\n",
    "    window_end = obs_date + pd.Timedelta(days=10)\n",
    "    churned_users = get_churned_users(df_train, obs_date, window_end)\n",
    "    \n",
    "    # Convert to Series with same index as features\n",
    "    labels = pd.Series(\n",
    "        features.index.isin(churned_users).astype(int),\n",
    "        index=features.index,\n",
    "        name='churned'\n",
    "    )\n",
    "    \n",
    "    X_train_list.append(features)\n",
    "    y_train_list.append(labels)\n",
    "    \n",
    "    print(f\"Obs date: {obs_date.date()}, Users: {len(features)}, Churn rate: {labels.mean():.2%}\")\n",
    "\n",
    "# Combine all training samples\n",
    "X_train_combined = pd.concat(X_train_list)\n",
    "y_train_combined = pd.concat(y_train_list)\n",
    "\n",
    "# Drop non-feature columns\n",
    "feature_cols = X_train_combined.select_dtypes(include=[np.number]).columns\n",
    "feature_cols = [c for c in feature_cols if c not in ['registration', 'ts_min', 'ts_max', 'total_length']]\n",
    "X_train_final = X_train_combined[feature_cols]\n",
    "\n",
    "test_features = aggregate_features(df_test, '2018-11-20')\n",
    "X_test = test_features[feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1cfb858-3204-448d-b5db-5b75a22b6010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training samples: 88369\n",
      "Overall churn rate: 4.44%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=13, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_final, y_train_combined)\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(X_train_final)}\")\n",
    "print(f\"Overall churn rate: {y_train_combined.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84171d09-fc52-4915-9543-232843d5f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base predicted churn: 8.33%\n",
      "Predicted churn at 0.4 threshold: 28.17%\n",
      "Submission saved to frog2.csv\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate_model\n",
    "\n",
    "evaluate_model(model, X_test, p=0.4, file_out='frog2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953e37a-bbce-42db-b7e0-03d188083c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
